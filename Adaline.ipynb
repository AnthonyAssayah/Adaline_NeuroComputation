{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnthonyAssayah/Adaline_NeuroComputation/blob/main/Adaline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "32KiNy6NEoRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74f4c72-75fb-41be-a1bd-d6d2f5838ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      label                                             vector\n",
            "0         1  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "1         1  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "2         1  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "3         2  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "4         2  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "...     ...                                                ...\n",
            "2461      1  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "2462      1  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "2463      3  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "2464      3  [-1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1...\n",
            "2465      3  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n",
            "\n",
            "[2466 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Function to check if the file has the correct structure\n",
        "def is_correct_structure(file_path):\n",
        "  with open(file_path, 'r') as f:\n",
        "      lines = f.readlines()\n",
        "      if len(lines) != 3:\n",
        "        return False\n",
        "      for line in lines:\n",
        "        try:\n",
        "          values = eval(line.strip())\n",
        "        except:\n",
        "          continue\n",
        "        if len(values) != 101 or values[0] not in [1, 2, 3]:\n",
        "          return False\n",
        "  return True\n",
        "\n",
        "# Function to load data from a file and return a list of tuples (label, vector)\n",
        "def load_data_from_file(file_path):\n",
        "  data = []\n",
        "  with open(file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "      try:\n",
        "        values = eval(line.strip())\n",
        "      except:\n",
        "        continue\n",
        "      label = int(values[0])\n",
        "      vector = list(values[1:])\n",
        "      data.append((label, vector))\n",
        "  return data\n",
        "\n",
        "# Main function to load all txt files in a directory and make a pandas dataframe\n",
        "def load_data_from_directory(dir_path):\n",
        "  all_data = []\n",
        "  for file in os.listdir(dir_path):\n",
        "    file_path = os.path.join(dir_path, file)\n",
        "    if file.endswith('.txt') and is_correct_structure(file_path):\n",
        "      all_data.extend(load_data_from_file(file_path))\n",
        "  \n",
        "  df = pd.DataFrame(all_data, columns=['label', 'vector'])\n",
        "  return df\n",
        "\n",
        "# Example usage\n",
        "# Example usage\n",
        "dir_path_1 = 'vec1'\n",
        "dir_path_2 = 'vec2'\n",
        "\n",
        "dataframe_1 = load_data_from_directory(dir_path_1)\n",
        "dataframe_2 = load_data_from_directory(dir_path_2)\n",
        "\n",
        "# Combine dataframes from both directories\n",
        "combined_dataframe = pd.concat([dataframe_1, dataframe_2], ignore_index=True)\n",
        "print(combined_dataframe)\n",
        "\n",
        "# Save the dataframe to a CSV file\n",
        "combined_dataframe.to_csv('combined_dataframe.csv', index=False)\n",
        "\n",
        "# Save the dataframe to a Pickle file\n",
        "combined_dataframe.to_pickle('combined_dataframe.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Adaline(object):\n",
        "    # Initializing the learning rate, epoch, weights, and costs\n",
        "    def __init__(self, rate=0.001, epoch=50):\n",
        "        self.l_rate = rate\n",
        "        self.epoch = epoch\n",
        "        self.weights = [] # weights will be initialized in the `fit` method\n",
        "        self.costs = [] # empty list to store the cost of each epoch\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    def fit(self, X, y):\n",
        "        # Get the number of rows and columns in the data\n",
        "        row = X.shape[0]\n",
        "        col = X.shape[1]\n",
        "        # Adding bias to the data\n",
        "        X = self._bias(X, (row, col))\n",
        "        # Initializing the weights with random values\n",
        "        np.random.seed(1)\n",
        "        self.weights = np.random.rand(col + 1)\n",
        "        # Training the model\n",
        "        for epoch in range(self.epoch):\n",
        "            # Shuffle the data randomly\n",
        "            X, y = self._shuffle(X, y)\n",
        "            cost = []\n",
        "            # Loop through each sample in the data and update the weights\n",
        "            for sample, label in zip(X, y):\n",
        "                cost.append(self._update_weights(sample, label))\n",
        "            # Compute the average cost for this epoch and append it to the costs list\n",
        "            avg = sum(cost) / len(y)\n",
        "            self.costs.append(avg)\n",
        "        return self\n",
        "\n",
        "    # Updating the weights\n",
        "    def _update_weights(self, sample, label):\n",
        "        # Calculate the output of the neuron\n",
        "        result = self.net_input(sample)\n",
        "        # Calculate the error\n",
        "        error = label - result\n",
        "        # Update the weights using the formula\n",
        "        # weights = weights + learning_rate * input * error\n",
        "        self.weights += self.l_rate * sample.dot(error)\n",
        "        # Calculate the cost for this sample and return it\n",
        "        return (error ** 2) / 2\n",
        "\n",
        "    # Shuffling the data randomly\n",
        "    def _shuffle(self, X, y):\n",
        "        # Generate a random permutation of the indices\n",
        "        per = np.random.permutation(len(y))\n",
        "        # Return the shuffled data\n",
        "        return X[per], y[per]\n",
        "\n",
        "    # Calculating the net input using matrix multiplication\n",
        "    def net_input(self, X):\n",
        "        return np.dot(X, self.weights)\n",
        "\n",
        "    # Predicting the class labels for the test data\n",
        "    def predict(self, X):\n",
        "        # Add bias to the test data if necessary\n",
        "        if len(X.T) != len(self.weights):\n",
        "            X = self._bias(X, (X.shape[0], X.shape[1]))\n",
        "        # Calculate the net input for the test data\n",
        "        net_input = self.net_input(X)\n",
        "        # Return the predicted class labels using the sign function\n",
        "        return np.where(net_input > 0.0, 1, -1)\n",
        "\n",
        "    # Adding bias to the data\n",
        "    def _bias(self, X, size):\n",
        "        # Create an array of ones with the same number of rows as the data and one more column\n",
        "        bias = np.ones((size[0], size[1] + 1))\n",
        "        # Set the values in the new array to be equal to the original data\n",
        "        bias[:, 1:] = X\n",
        "        # Return the new array with bias added to the data\n",
        "        return bias\n",
        "\n",
        "\n",
        "# Function print confusion matrix \n",
        "def plot_confusion_matrix(predictions, y_test):\n",
        "    cm = confusion_matrix(predictions, y_test)\n",
        "    plt.subplots()\n",
        "    sns.heatmap(cm, fmt=\".0f\", annot=True)\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.xlabel(\"Actual label\")\n",
        "    plt.ylabel(\"Predicted label\")\n",
        "\n",
        "\n",
        "def classify(X, y, label_pair):\n",
        "    label1, label2 = label_pair\n",
        "\n",
        "    # Filter the dataframe to include only the specified labels\n",
        "    dataframe = pd.DataFrame({'vector': list(X), 'label': list(y)})\n",
        "    dataframe = dataframe[dataframe['label'].isin(label_pair)]\n",
        "\n",
        "    # Prepare the input data (X) and labels (y)\n",
        "    X = np.array([np.array(x) for x in dataframe['vector']])\n",
        "    y = np.array(dataframe['label'])\n",
        "\n",
        "    # Convert labels to -1 and 1\n",
        "    y = np.where(y == label1, -1, 1)\n",
        "    y = np.where(y == label2, 1, y)\n",
        "\n",
        "    # Normalize the input data\n",
        "    X = (X - X.mean()) / X.std()\n",
        "\n",
        "    # Cross-validate the Adaline classifier\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "    accuracies = []\n",
        "    for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        adaline = Adaline(rate=0.001, epoch=50)\n",
        "        adaline.fit(X_train, y_train)\n",
        "        predictions = adaline.predict(X_test)\n",
        "        accuracy = np.mean(predictions == y_test)\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Iteration {i+1}: Test accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        # # Print confusion matrix and classification report\n",
        "        # cm = plot_confusion_matrix(predictions, y_test)\n",
        "        # cr = classification_report(y_test, predictions, target_names=['Negative', 'Positive'])\n",
        "        # print(f\"\\nClassification report:\\n{cr}\\n\")\n",
        "\n",
        "    # Calculate the average accuracy and standard deviation\n",
        "    avg_accuracy = np.mean(accuracies)\n",
        "    std_deviation = np.std(accuracies)\n",
        "    print(f\"Average accuracy: {avg_accuracy * 100:.2f}%\")\n",
        "    print(f\"Standard deviation: {std_deviation * 100:.2f}%\\n\")\n",
        "    return avg_accuracy, std_deviation\n",
        "\n"
      ],
      "metadata": {
        "id": "M0Yws2Sp_cSA"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Load the saved dataframe\n",
        "    dataframe = pd.read_pickle('combined_dataframe.pkl')\n",
        "\n",
        "    # Classify 'ב' vs 'מ'\n",
        "    print(\"************* Classify 'ב' vs 'מ' *************\")\n",
        "    label_pair = [1, 2]\n",
        "    classify(dataframe['vector'], dataframe['label'], label_pair)\n",
        "\n",
        "    # Classify 'ב' vs 'ל'\n",
        "    print(\"************* Classify 'ב' vs 'ל' *************\")\n",
        "    label_pair = [1, 3]\n",
        "    classify(dataframe['vector'], dataframe['label'], label_pair)\n",
        "\n",
        "    # Classify 'מ' vs 'ל'\n",
        "    print(\"************* Classify 'מ' vs 'ל' *************\")\n",
        "    label_pair = [2, 3]\n",
        "    classify(dataframe['vector'], dataframe['label'], label_pair)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAUHIiym_qgG",
        "outputId": "aa6feb74-a27b-4bd9-e009-e63a477286c2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************* Classify 'ב' vs 'מ' *************\n",
            "Iteration 1: Test accuracy: 90.91%\n",
            "Iteration 2: Test accuracy: 84.55%\n",
            "Iteration 3: Test accuracy: 87.54%\n",
            "Iteration 4: Test accuracy: 86.63%\n",
            "Iteration 5: Test accuracy: 88.15%\n",
            "Average accuracy: 87.55%\n",
            "Standard deviation: 2.07%\n",
            "\n",
            "************* Classify 'ב' vs 'ל' *************\n",
            "Iteration 1: Test accuracy: 83.28%\n",
            "Iteration 2: Test accuracy: 78.35%\n",
            "Iteration 3: Test accuracy: 79.88%\n",
            "Iteration 4: Test accuracy: 81.71%\n",
            "Iteration 5: Test accuracy: 81.10%\n",
            "Average accuracy: 80.86%\n",
            "Standard deviation: 1.67%\n",
            "\n",
            "************* Classify 'מ' vs 'ל' *************\n",
            "Iteration 1: Test accuracy: 83.89%\n",
            "Iteration 2: Test accuracy: 84.19%\n",
            "Iteration 3: Test accuracy: 87.54%\n",
            "Iteration 4: Test accuracy: 87.23%\n",
            "Iteration 5: Test accuracy: 88.41%\n",
            "Average accuracy: 86.25%\n",
            "Standard deviation: 1.85%\n",
            "\n"
          ]
        }
      ]
    }
  ]
}